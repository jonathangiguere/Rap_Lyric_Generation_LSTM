{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21247,
     "status": "ok",
     "timestamp": 1586060940211,
     "user": {
      "displayName": "Jesse Borg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJTIY1G60GY_OjYcCPZt5JdJCM6kkXTGAgRobe=s64",
      "userId": "14131079589536879976"
     },
     "user_tz": 240
    },
    "id": "hlntePnrksed",
    "outputId": "978aa160-c7e5-4e89-9e23-46bd9abe5aa2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MXCU53dUkdmU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load Packages\n",
    "import tensorflow\n",
    "from tensorflow.keras import backend\n",
    "#from __future__ import print_function\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import load_model, save_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4O9E32UmkwNt"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/content/drive/My Drive/Colab Notebooks/NLP Group Project/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-484567d59060>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive/My Drive/Colab Notebooks/NLP Group Project/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/content/drive/My Drive/Colab Notebooks/NLP Group Project/'"
     ]
    }
   ],
   "source": [
    "os.chdir('/content/drive/My Drive/Colab Notebooks/NLP Group Project/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-2y6o1JkmSp"
   },
   "outputs": [],
   "source": [
    "# Read Songs\n",
    "songs = pd.read_csv('drake-songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1586061286506,
     "user": {
      "displayName": "Jesse Borg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJTIY1G60GY_OjYcCPZt5JdJCM6kkXTGAgRobe=s64",
      "userId": "14131079589536879976"
     },
     "user_tz": 240
    },
    "id": "ns3F69xMlCZF",
    "outputId": "6b46a81d-2c15-4dc5-dddb-3c1748c2d05e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367372"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''\n",
    "\n",
    "for index, row in songs['lyrics'].iteritems():\n",
    "    cleaned = str(row).lower().replace(' ', '\\n')\n",
    "    text = text + \" \".join(re.findall(r\"[a-z']+\", cleaned))\n",
    "    \n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"money money cars cars clothes clothes the hoes i suppose yeah i want the money money and the cars cars and the clothes the hoes i suppose i just wanna be i just wanna be successful i just wanna be i just wanna be successful i just wanna be i just wanna be successful drizzy ah yeah trey i fuckin' feel you they be starin' at the money like it's unfamiliar i get it i live it to me there's nothings realer just enough to solve your problems too much will kill ya and when i leave i always come right back here the young spit'a that everybody in rap fear a lot of y'all are still soundin' like last year the game needs change and i'm the mofucking cashier nickels for my thoughts dimes in my bed quarters of the kush shape the lines in my head take my verses too serious ya hate me 'cause i'm the one to paint a vivid picture no hd yeah i want it all that's why i strive for it dis me and you'll never hear a reply for it any awards show or party i get fly for it i know that it's comin' i just hope th\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 593,
     "status": "ok",
     "timestamp": 1586061287975,
     "user": {
      "displayName": "Jesse Borg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJTIY1G60GY_OjYcCPZt5JdJCM6kkXTGAgRobe=s64",
      "userId": "14131079589536879976"
     },
     "user_tz": 240
    },
    "id": "x6tibuXslDAl",
    "outputId": "18eebd3e-d575-4446-df9b-7a9ea780f39c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 5865\n"
     ]
    }
   ],
   "source": [
    "tokens = text.replace(\"'\", '').split(' ')\n",
    "\n",
    "chars = sorted(list(set(tokens)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "del char_indices['']\n",
    "del indices_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['money',\n",
       " 'money',\n",
       " 'cars',\n",
       " 'cars',\n",
       " 'clothes',\n",
       " 'clothes',\n",
       " 'the',\n",
       " 'hoes',\n",
       " 'i',\n",
       " 'suppose',\n",
       " 'yeah',\n",
       " 'i',\n",
       " 'want',\n",
       " 'the',\n",
       " 'money',\n",
       " 'money',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cars',\n",
       " 'cars',\n",
       " 'and',\n",
       " 'the',\n",
       " 'clothes',\n",
       " 'the',\n",
       " 'hoes',\n",
       " 'i',\n",
       " 'suppose',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'drizzy',\n",
       " 'ah',\n",
       " 'yeah',\n",
       " 'trey',\n",
       " 'i',\n",
       " 'fuckin',\n",
       " 'feel',\n",
       " 'you',\n",
       " 'they',\n",
       " 'be',\n",
       " 'starin',\n",
       " 'at',\n",
       " 'the',\n",
       " 'money',\n",
       " 'like',\n",
       " 'its',\n",
       " 'unfamiliar',\n",
       " 'i',\n",
       " 'get',\n",
       " 'it',\n",
       " 'i',\n",
       " 'live',\n",
       " 'it',\n",
       " 'to',\n",
       " 'me',\n",
       " 'theres',\n",
       " 'nothings',\n",
       " 'realer',\n",
       " 'just',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'solve',\n",
       " 'your',\n",
       " 'problems',\n",
       " 'too',\n",
       " 'much',\n",
       " 'will',\n",
       " 'kill',\n",
       " 'ya',\n",
       " 'and',\n",
       " 'when',\n",
       " 'i',\n",
       " 'leave',\n",
       " 'i',\n",
       " 'always',\n",
       " 'come',\n",
       " 'right',\n",
       " 'back',\n",
       " 'here',\n",
       " 'the',\n",
       " 'young',\n",
       " 'spita',\n",
       " 'that',\n",
       " 'everybody',\n",
       " 'in',\n",
       " 'rap',\n",
       " 'fear',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'yall',\n",
       " 'are',\n",
       " 'still',\n",
       " 'soundin',\n",
       " 'like',\n",
       " 'last',\n",
       " 'year',\n",
       " 'the',\n",
       " 'game',\n",
       " 'needs',\n",
       " 'change',\n",
       " 'and',\n",
       " 'im',\n",
       " 'the',\n",
       " 'mofucking',\n",
       " 'cashier',\n",
       " 'nickels',\n",
       " 'for',\n",
       " 'my',\n",
       " 'thoughts',\n",
       " 'dimes',\n",
       " 'in',\n",
       " 'my',\n",
       " 'bed',\n",
       " 'quarters',\n",
       " 'of',\n",
       " 'the',\n",
       " 'kush',\n",
       " 'shape',\n",
       " 'the',\n",
       " 'lines',\n",
       " 'in',\n",
       " 'my',\n",
       " 'head',\n",
       " 'take',\n",
       " 'my',\n",
       " 'verses',\n",
       " 'too',\n",
       " 'serious',\n",
       " 'ya',\n",
       " 'hate',\n",
       " 'me',\n",
       " 'cause',\n",
       " 'im',\n",
       " 'the',\n",
       " 'one',\n",
       " 'to',\n",
       " 'paint',\n",
       " 'a',\n",
       " 'vivid',\n",
       " 'picture',\n",
       " 'no',\n",
       " 'hd',\n",
       " 'yeah',\n",
       " 'i',\n",
       " 'want',\n",
       " 'it',\n",
       " 'all',\n",
       " 'thats',\n",
       " 'why',\n",
       " 'i',\n",
       " 'strive',\n",
       " 'for',\n",
       " 'it',\n",
       " 'dis',\n",
       " 'me',\n",
       " 'and',\n",
       " 'youll',\n",
       " 'never',\n",
       " 'hear',\n",
       " 'a',\n",
       " 'reply',\n",
       " 'for',\n",
       " 'it',\n",
       " 'any',\n",
       " 'awards',\n",
       " 'show',\n",
       " 'or',\n",
       " 'party',\n",
       " 'i',\n",
       " 'get',\n",
       " 'fly',\n",
       " 'for',\n",
       " 'it',\n",
       " 'i',\n",
       " 'know',\n",
       " 'that',\n",
       " 'its',\n",
       " 'comin',\n",
       " 'i',\n",
       " 'just',\n",
       " 'hope',\n",
       " 'that',\n",
       " 'im',\n",
       " 'alive',\n",
       " 'for',\n",
       " 'it',\n",
       " 'i',\n",
       " 'want',\n",
       " 'the',\n",
       " 'money',\n",
       " 'money',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cars',\n",
       " 'cars',\n",
       " 'and',\n",
       " 'the',\n",
       " 'clothes',\n",
       " 'the',\n",
       " 'hoes',\n",
       " 'i',\n",
       " 'suppose',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'yeah',\n",
       " 'i',\n",
       " 'want',\n",
       " 'things',\n",
       " 'to',\n",
       " 'go',\n",
       " 'my',\n",
       " 'way',\n",
       " 'but',\n",
       " 'as',\n",
       " 'of',\n",
       " 'late',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'shit',\n",
       " 'been',\n",
       " 'goin',\n",
       " 'sideways',\n",
       " 'and',\n",
       " 'my',\n",
       " 'mother',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'runaway',\n",
       " 'from',\n",
       " 'home',\n",
       " 'but',\n",
       " 'i',\n",
       " 'left',\n",
       " 'somethin',\n",
       " 'in',\n",
       " 'the',\n",
       " 'car',\n",
       " 'and',\n",
       " 'so',\n",
       " 'i',\n",
       " 'caught',\n",
       " 'her',\n",
       " 'in',\n",
       " 'the',\n",
       " 'driveway',\n",
       " 'and',\n",
       " 'she',\n",
       " 'cried',\n",
       " 'to',\n",
       " 'me',\n",
       " 'so',\n",
       " 'i',\n",
       " 'cried',\n",
       " 'too',\n",
       " 'and',\n",
       " 'my',\n",
       " 'stomach',\n",
       " 'was',\n",
       " 'soakin',\n",
       " 'wet',\n",
       " 'she',\n",
       " 'only',\n",
       " 'and',\n",
       " 'forty',\n",
       " 'eight',\n",
       " 'hours',\n",
       " 'was',\n",
       " 'all',\n",
       " 'before',\n",
       " 'i',\n",
       " 'showed',\n",
       " 'up',\n",
       " 'and',\n",
       " 'brought',\n",
       " 'a',\n",
       " 'thousand',\n",
       " 'dollars',\n",
       " 'worth',\n",
       " 'of',\n",
       " 'drinks',\n",
       " 'and',\n",
       " 'got',\n",
       " 'pulled',\n",
       " 'up',\n",
       " 'damn',\n",
       " 'my',\n",
       " 'reality',\n",
       " 'just',\n",
       " 'set',\n",
       " 'in',\n",
       " 'and',\n",
       " 'even',\n",
       " 'when',\n",
       " 'the',\n",
       " 'phantoms',\n",
       " 'leased',\n",
       " 'them',\n",
       " 'hoes',\n",
       " 'wanna',\n",
       " 'get',\n",
       " 'in',\n",
       " 'i',\n",
       " 'do',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'of',\n",
       " 'things',\n",
       " 'hopin',\n",
       " 'i',\n",
       " 'neva',\n",
       " 'have',\n",
       " 'to',\n",
       " 'fit',\n",
       " 'in',\n",
       " 'so',\n",
       " 'tryin',\n",
       " 'to',\n",
       " 'keep',\n",
       " 'up',\n",
       " 'with',\n",
       " 'my',\n",
       " 'progress',\n",
       " 'is',\n",
       " 'like',\n",
       " 'a',\n",
       " 'dead',\n",
       " 'end',\n",
       " 'my',\n",
       " 'girl',\n",
       " 'love',\n",
       " 'me',\n",
       " 'but',\n",
       " 'fuck',\n",
       " 'it',\n",
       " 'my',\n",
       " 'heart',\n",
       " 'beat',\n",
       " 'slow',\n",
       " 'and',\n",
       " 'right',\n",
       " 'now',\n",
       " 'the',\n",
       " 'tour',\n",
       " 'bus',\n",
       " 'is',\n",
       " 'lookin',\n",
       " 'like',\n",
       " 'a',\n",
       " 'freak',\n",
       " 'show',\n",
       " 'and',\n",
       " 'life',\n",
       " 'change',\n",
       " 'for',\n",
       " 'us',\n",
       " 'every',\n",
       " 'single',\n",
       " 'week',\n",
       " 'so',\n",
       " 'its',\n",
       " 'good',\n",
       " 'but',\n",
       " 'i',\n",
       " 'know',\n",
       " 'this',\n",
       " 'aint',\n",
       " 'the',\n",
       " 'peak',\n",
       " 'though',\n",
       " 'cause',\n",
       " 'i',\n",
       " 'want',\n",
       " 'i',\n",
       " 'want',\n",
       " 'the',\n",
       " 'money',\n",
       " 'money',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cars',\n",
       " 'cars',\n",
       " 'and',\n",
       " 'the',\n",
       " 'clothes',\n",
       " 'the',\n",
       " 'hoes',\n",
       " 'i',\n",
       " 'suppose',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'wise',\n",
       " 'words',\n",
       " 'from',\n",
       " 'a',\n",
       " 'decent',\n",
       " 'man',\n",
       " 'back',\n",
       " 'when',\n",
       " 'i',\n",
       " 'was',\n",
       " 'tryin',\n",
       " 'to',\n",
       " 'put',\n",
       " 'a',\n",
       " 'ring',\n",
       " 'on',\n",
       " 'alicia',\n",
       " 'hand',\n",
       " 'this',\n",
       " 'lost',\n",
       " 'boy',\n",
       " 'got',\n",
       " 'fly',\n",
       " 'without',\n",
       " 'peter',\n",
       " 'pan',\n",
       " 'and',\n",
       " 'my',\n",
       " 'delivery',\n",
       " 'just',\n",
       " 'got',\n",
       " 'me',\n",
       " 'buzzin',\n",
       " 'like',\n",
       " 'the',\n",
       " 'pizza',\n",
       " 'man',\n",
       " 'in',\n",
       " 'person',\n",
       " 'i',\n",
       " 'am',\n",
       " 'everything',\n",
       " 'and',\n",
       " 'more',\n",
       " 'im',\n",
       " 'everywhere',\n",
       " 'these',\n",
       " 'other',\n",
       " 'niggas',\n",
       " 'never',\n",
       " 'been',\n",
       " 'before',\n",
       " 'but',\n",
       " 'inside',\n",
       " 'im',\n",
       " 'treadin',\n",
       " 'waters',\n",
       " 'steady',\n",
       " 'tryin',\n",
       " 'to',\n",
       " 'swim',\n",
       " 'ashore',\n",
       " 'im',\n",
       " 'on',\n",
       " 'a',\n",
       " 'shoppin',\n",
       " 'spree',\n",
       " 'to',\n",
       " 'get',\n",
       " 'whateva',\n",
       " 'is',\n",
       " 'in',\n",
       " 'store',\n",
       " 'yeah',\n",
       " 'just',\n",
       " 'call',\n",
       " 'me',\n",
       " 'shop',\n",
       " 'and',\n",
       " 'bag',\n",
       " 'drizzy',\n",
       " 'and',\n",
       " 'call',\n",
       " 'me',\n",
       " 'mr',\n",
       " 'damn',\n",
       " 'he',\n",
       " 'aint',\n",
       " 'copin',\n",
       " 'that',\n",
       " 'is',\n",
       " 'he',\n",
       " 'and',\n",
       " 'fans',\n",
       " 'of',\n",
       " 'these',\n",
       " 'freshman',\n",
       " 'is',\n",
       " 'about',\n",
       " 'to',\n",
       " 'get',\n",
       " 'iffy',\n",
       " 'while',\n",
       " 'this',\n",
       " 'youngin',\n",
       " 'that',\n",
       " 'you',\n",
       " 'doubtin',\n",
       " 'is',\n",
       " 'about',\n",
       " 'to',\n",
       " 'get',\n",
       " 'busy',\n",
       " 'ima',\n",
       " 'kill',\n",
       " 'it',\n",
       " 'i',\n",
       " 'promise',\n",
       " 'this',\n",
       " 'i',\n",
       " 'know',\n",
       " 'you',\n",
       " 'mad',\n",
       " 'ive',\n",
       " 'always',\n",
       " 'treated',\n",
       " 'my',\n",
       " 'city',\n",
       " 'like',\n",
       " 'some',\n",
       " 'shoulder',\n",
       " 'pads',\n",
       " 'to',\n",
       " 'big',\n",
       " 'homie',\n",
       " 'use',\n",
       " 'a',\n",
       " 'flash',\n",
       " 'if',\n",
       " 'you',\n",
       " 'must',\n",
       " 'and',\n",
       " 'i',\n",
       " 'swear',\n",
       " 'i',\n",
       " 'aint',\n",
       " 'askin',\n",
       " 'for',\n",
       " 'much',\n",
       " 'all',\n",
       " 'i',\n",
       " 'want',\n",
       " 'is',\n",
       " 'i',\n",
       " 'want',\n",
       " 'the',\n",
       " 'money',\n",
       " 'money',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cars',\n",
       " 'cars',\n",
       " 'and',\n",
       " 'the',\n",
       " 'clothes',\n",
       " 'the',\n",
       " 'hoes',\n",
       " 'i',\n",
       " 'suppose',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'i',\n",
       " 'just',\n",
       " 'wanna',\n",
       " 'be',\n",
       " 'successful',\n",
       " 'its',\n",
       " 'like',\n",
       " 'i',\n",
       " 'know',\n",
       " 'what',\n",
       " 'i',\n",
       " 'got',\n",
       " 'to',\n",
       " 'say',\n",
       " 'i',\n",
       " 'just',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'how',\n",
       " 'to',\n",
       " 'say',\n",
       " 'it',\n",
       " 'to',\n",
       " 'you',\n",
       " 'pardon',\n",
       " 'the',\n",
       " 'swag',\n",
       " 'but',\n",
       " 'bitches',\n",
       " 'cartate',\n",
       " 'long',\n",
       " 'bread',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'eat',\n",
       " 'shortcake',\n",
       " 'how',\n",
       " 'come',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'miss',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'like',\n",
       " 'i',\n",
       " 'cant',\n",
       " 'miss',\n",
       " 'court',\n",
       " 'dates',\n",
       " 'cheese',\n",
       " 'but',\n",
       " 'shes',\n",
       " 'not',\n",
       " 'in',\n",
       " 'this',\n",
       " 'portrait',\n",
       " 'lifes',\n",
       " 'fine',\n",
       " 'but',\n",
       " 'i',\n",
       " 'do',\n",
       " 'not',\n",
       " 'portray',\n",
       " 'im',\n",
       " 'on',\n",
       " 'the',\n",
       " 'other',\n",
       " 'side',\n",
       " 'but',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'sharp',\n",
       " 'gate',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'the',\n",
       " 'glow',\n",
       " 'i',\n",
       " 'want',\n",
       " 'the',\n",
       " 'gloray',\n",
       " 'and',\n",
       " 'ima',\n",
       " 'fuck',\n",
       " 'the',\n",
       " 'world',\n",
       " 'but',\n",
       " 'this',\n",
       " 'is',\n",
       " 'just',\n",
       " 'foreplay',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'hearing',\n",
       " 'bullshit',\n",
       " 'bring',\n",
       " 'her',\n",
       " 'on',\n",
       " 'to',\n",
       " 'cow',\n",
       " 'shit',\n",
       " 'havent',\n",
       " 'met',\n",
       " 'a',\n",
       " 'smell',\n",
       " 'thats',\n",
       " 'stinkier',\n",
       " 'than',\n",
       " 'shit',\n",
       " 'thats',\n",
       " 'word',\n",
       " 'to',\n",
       " 'toronto',\n",
       " 'so',\n",
       " 'high',\n",
       " 'up',\n",
       " 'i',\n",
       " 'got',\n",
       " 'birds',\n",
       " 'in',\n",
       " 'the',\n",
       " 'condo',\n",
       " 'aint',\n",
       " 'that',\n",
       " 'a',\n",
       " 'female',\n",
       " 'dog',\n",
       " 'ask',\n",
       " 'her',\n",
       " 'who',\n",
       " 'i',\n",
       " 'am',\n",
       " 'to',\n",
       " 'her',\n",
       " 'and',\n",
       " 'she',\n",
       " 'yell',\n",
       " 'god',\n",
       " 'weezy',\n",
       " 'baby',\n",
       " 'i',\n",
       " 'go',\n",
       " 'real',\n",
       " 'hard',\n",
       " 'no',\n",
       " 'further',\n",
       " 'details',\n",
       " 'boyyou',\n",
       " 'know',\n",
       " 'alot',\n",
       " 'of',\n",
       " 'girls',\n",
       " 'be',\n",
       " 'thinkin',\n",
       " 'my',\n",
       " 'songs',\n",
       " 'are',\n",
       " 'about',\n",
       " 'them',\n",
       " 'but',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'to',\n",
       " 'get',\n",
       " 'confused',\n",
       " 'this',\n",
       " 'ones',\n",
       " 'for',\n",
       " 'you',\n",
       " 'baby',\n",
       " 'you',\n",
       " 'my',\n",
       " 'everything',\n",
       " 'you',\n",
       " 'all',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'wanted',\n",
       " 'we',\n",
       " 'could',\n",
       " 'do',\n",
       " 'it',\n",
       " 'real',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'than',\n",
       " 'you',\n",
       " 'ever',\n",
       " 'done',\n",
       " 'it',\n",
       " 'youll',\n",
       " 'be',\n",
       " 'up',\n",
       " 'on',\n",
       " 'everything',\n",
       " 'other',\n",
       " 'hoes',\n",
       " 'aint',\n",
       " 'ever',\n",
       " 'on',\n",
       " 'it',\n",
       " 'i',\n",
       " 'want',\n",
       " 'this',\n",
       " 'forever',\n",
       " 'i',\n",
       " 'swear',\n",
       " 'i',\n",
       " 'can',\n",
       " 'spend',\n",
       " 'whatever',\n",
       " 'on',\n",
       " 'it',\n",
       " 'cause',\n",
       " 'she',\n",
       " 'hold',\n",
       " 'me',\n",
       " 'down',\n",
       " 'every',\n",
       " 'time',\n",
       " 'i',\n",
       " 'hit',\n",
       " 'her',\n",
       " 'up',\n",
       " 'when',\n",
       " 'i',\n",
       " 'get',\n",
       " 'right',\n",
       " 'i',\n",
       " 'promise',\n",
       " 'that',\n",
       " 'we',\n",
       " 'gon',\n",
       " 'live',\n",
       " 'it',\n",
       " 'up',\n",
       " 'she',\n",
       " 'made',\n",
       " 'me',\n",
       " 'beg',\n",
       " 'for',\n",
       " 'it',\n",
       " 'til',\n",
       " 'she',\n",
       " 'give',\n",
       " 'it',\n",
       " 'up',\n",
       " 'and',\n",
       " 'i',\n",
       " 'say',\n",
       " 'the',\n",
       " 'same',\n",
       " 'thing',\n",
       " 'every',\n",
       " 'single',\n",
       " 'time',\n",
       " 'i',\n",
       " 'say',\n",
       " 'you',\n",
       " 'the',\n",
       " 'fuckin',\n",
       " 'best',\n",
       " 'you',\n",
       " 'the',\n",
       " 'fuckin',\n",
       " 'best',\n",
       " 'you',\n",
       " 'the',\n",
       " 'fuckin',\n",
       " 'best',\n",
       " 'you',\n",
       " 'the',\n",
       " 'fuckin',\n",
       " 'best',\n",
       " 'you',\n",
       " 'the',\n",
       " 'best',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'best',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'best',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'best',\n",
       " 'i',\n",
       " 'ever',\n",
       " 'had',\n",
       " 'i',\n",
       " 'say',\n",
       " 'you',\n",
       " 'the',\n",
       " 'fuckin',\n",
       " 'know',\n",
       " 'you',\n",
       " 'got',\n",
       " 'a',\n",
       " 'roommate',\n",
       " 'call',\n",
       " 'me',\n",
       " 'when',\n",
       " 'theres',\n",
       " 'no',\n",
       " 'one',\n",
       " 'there',\n",
       " 'put',\n",
       " 'the',\n",
       " 'key',\n",
       " 'under',\n",
       " 'the',\n",
       " 'mat',\n",
       " 'you',\n",
       " 'know',\n",
       " 'ill',\n",
       " 'be',\n",
       " 'over',\n",
       " 'there',\n",
       " 'yup',\n",
       " 'ill',\n",
       " 'be',\n",
       " 'over',\n",
       " 'there',\n",
       " 'shorty',\n",
       " 'ill',\n",
       " 'be',\n",
       " 'over',\n",
       " 'there',\n",
       " 'ill',\n",
       " 'be',\n",
       " 'hittin',\n",
       " 'all',\n",
       " 'the',\n",
       " 'spots',\n",
       " 'that',\n",
       " 'you',\n",
       " 'aint']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 15\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(tokens)-maxlen-5, 1):\n",
    "    sentences.append(tokens[i:i+maxlen])\n",
    "    next_chars.append(tokens[i+maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OgKMP2MAlJIh",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-e8540d0fc1e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnext_chars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1047,
     "status": "ok",
     "timestamp": 1586061450801,
     "user": {
      "displayName": "Jesse Borg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJTIY1G60GY_OjYcCPZt5JdJCM6kkXTGAgRobe=s64",
      "userId": "14131079589536879976"
     },
     "user_tz": 240
    },
    "id": "rCwYtEISlLod",
    "outputId": "f18165ff-e4c7-4bce-823d-c1de72554019"
   },
   "outputs": [],
   "source": [
    "# check CuDNNLSTM on VM\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 15, 128)           3068928   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5865)              756585    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 5865)              0         \n",
      "=================================================================\n",
      "Total params: 3,957,097\n",
      "Trainable params: 3,957,097\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jeYNO7w6lQN5"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds[0], 5865)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CPzaNcoKlQRj"
   },
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs):\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(tokens) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.5, 0.8]:\n",
    "        print('----- diversity:', diversity)\n",
    "        \n",
    "        generated = ''\n",
    "        sentence = tokens[start_index: start_index + maxlen]\n",
    "        for i in sentence:\n",
    "            generated += str(i)\n",
    "            generated += ' '\n",
    "\n",
    "        sentence_list = ''\n",
    "        for i in sentence:\n",
    "            sentence_list += str(i) + ' '\n",
    "        print('----- Generating with seed: \"' + sentence_list + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(25):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence.append(next_char)\n",
    "            sentence.pop(0)\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.write(' ')\n",
    "\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1259478,
     "status": "ok",
     "timestamp": 1586062727793,
     "user": {
      "displayName": "Jesse Borg",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhJTIY1G60GY_OjYcCPZt5JdJCM6kkXTGAgRobe=s64",
      "userId": "14131079589536879976"
     },
     "user_tz": 240
    },
    "id": "JBE44EZSlQV9",
    "outputId": "cc013f95-0f66-4036-e08f-d0a140b27f61",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75057 samples\n",
      "Epoch 1/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.6696----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"the champagne your best friend is bar tending your parents sayin this another phase in \"\n",
      "the champagne your best friend is bar tending your parents sayin this another phase in i im i i the i the the i i im the i on the you i the the i the i i i you \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"the champagne your best friend is bar tending your parents sayin this another phase in \"\n",
      "the champagne your best friend is bar tending your parents sayin this another phase in so me couple girl i im a to soon too a how the i me you me pillow a i time like make following the \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"the champagne your best friend is bar tending your parents sayin this another phase in \"\n",
      "the champagne your best friend is bar tending your parents sayin this another phase in taste jealous sweetest only chyna h real your night store shut where fo fuck keep the threw forgetting to through these old tryin down i \n",
      "75057/75057 [==============================] - 213s 3ms/sample - loss: 1.6703\n",
      "Epoch 2/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5833----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"im just a rapper and soon shell have met another thats why me and lil \"\n",
      "im just a rapper and soon shell have met another thats why me and lil i i i i i i i i i i i i i i i i i i i i i i i i i \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"im just a rapper and soon shell have met another thats why me and lil \"\n",
      "im just a rapper and soon shell have met another thats why me and lil i a in to things the it hard big point probably i i i i baby and it the i it even a im it \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"im just a rapper and soon shell have met another thats why me and lil \"\n",
      "im just a rapper and soon shell have met another thats why me and lil wheels im in bent scheme best e im things wanna rich ya develop dirty a and story wish bond amazing you unfamiliar your bad the \n",
      "75057/75057 [==============================] - 205s 3ms/sample - loss: 1.5835\n",
      "Epoch 3/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5770----- Generating text after Epoch: 2\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"that your the one to blame tryna enjoy myself with taz in miami at the \"\n",
      "that your the one to blame tryna enjoy myself with taz in miami at the the i the the the the the the the the the i the the the the the i the the im the me the better \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"that your the one to blame tryna enjoy myself with taz in miami at the \"\n",
      "that your the one to blame tryna enjoy myself with taz in miami at the their how the find the you the im done superficial the the dont ever least til either go the the and drunk the instead and \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"that your the one to blame tryna enjoy myself with taz in miami at the \"\n",
      "that your the one to blame tryna enjoy myself with taz in miami at the im racking david bathroom when oh accept pokin jordan the hoes myself you im cant mine one and protest ill he tell all eyes with \n",
      "75057/75057 [==============================] - 215s 3ms/sample - loss: 1.5774\n",
      "Epoch 4/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5738----- Generating text after Epoch: 3\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"came up and forgot about your ass and thats some real shiti should be downtown \"\n",
      "came up and forgot about your ass and thats some real shiti should be downtown me and i i that i i i i i you i i you when i the i i i i you i i i \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"came up and forgot about your ass and thats some real shiti should be downtown \"\n",
      "came up and forgot about your ass and thats some real shiti should be downtown me you i got i at know tonight oh together i the i that the this i you will with your it your my young \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"came up and forgot about your ass and thats some real shiti should be downtown \"\n",
      "came up and forgot about your ass and thats some real shiti should be downtown college accept you handed height such mascot i boot you is profit it think life thoughts saying like make if and power have in im \n",
      "75057/75057 [==============================] - 209s 3ms/sample - loss: 1.5748\n",
      "Epoch 5/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5736----- Generating text after Epoch: 4\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"hit me up when its good for you ungratefuli wake up on a daily basis \"\n",
      "hit me up when its good for you ungratefuli wake up on a daily basis it i i you the i know i the i its it know i i you i i know you i i it i i \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"hit me up when its good for you ungratefuli wake up on a daily basis \"\n",
      "hit me up when its good for you ungratefuli wake up on a daily basis these you little of i hair girls it ill just i better in all fearin you going fireworks as you gave around they better the \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"hit me up when its good for you ungratefuli wake up on a daily basis \"\n",
      "hit me up when its good for you ungratefuli wake up on a daily basis even taking that speak taz k better finger i girl ive thoughts start bangin drinks ill real here it have dont died diplomas she seater \n",
      "75057/75057 [==============================] - 229s 3ms/sample - loss: 1.5736\n",
      "Epoch 6/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5696----- Generating text after Epoch: 5\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"and i still love her but it fell through because i wasnt ready and your \"\n",
      "and i still love her but it fell through because i wasnt ready and your you a i and a and and a a and and a a you and and i and and and and i and and to \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"and i still love her but it fell through because i wasnt ready and your \"\n",
      "and i still love her but it fell through because i wasnt ready and your do everything to dedicating me be and we her a and right right that about mean that you a i a i know to love \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"and i still love her but it fell through because i wasnt ready and your \"\n",
      "and i still love her but it fell through because i wasnt ready and your the down goes notice girl on you love h pardon it eyes be phone following married for or down often a loving live take getting \n",
      "75057/75057 [==============================] - 232s 3ms/sample - loss: 1.5702\n",
      "Epoch 7/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5707----- Generating text after Epoch: 6\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"turn i make some more money its got me delirious when your gang gangs in \"\n",
      "turn i make some more money its got me delirious when your gang gangs in you i they been the and you you the and the you you the you you and they you the the you and that and \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"turn i make some more money its got me delirious when your gang gangs in \"\n",
      "turn i make some more money its got me delirious when your gang gangs in is and happened girl so take is we the you it and know come wanna your you and i it no really and and still \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"turn i make some more money its got me delirious when your gang gangs in \"\n",
      "turn i make some more money its got me delirious when your gang gangs in for well days they should goes those is you steady crack fireworks fine you that very done strung all her pump is sake and magic \n",
      "75057/75057 [==============================] - 227s 3ms/sample - loss: 1.5709\n",
      "Epoch 8/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5704----- Generating text after Epoch: 7\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"something i would die for octobers on but its lookin like july four i just \"\n",
      "something i would die for octobers on but its lookin like july four i just the the the the the the the the the i i the i the the the the the the the the the the the the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"something i would die for octobers on but its lookin like july four i just \"\n",
      "something i would die for octobers on but its lookin like july four i just classic the here i girls and i the back heart get they the of to time the it the the the and that you the \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"something i would die for octobers on but its lookin like july four i just \"\n",
      "something i would die for octobers on but its lookin like july four i just i angry griffith probably me much a and me hoe okay fame be which intentions home but do bright i they that a a repeat \n",
      "75057/75057 [==============================] - 220s 3ms/sample - loss: 1.5700\n",
      "Epoch 9/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5692----- Generating text after Epoch: 8\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"to but i cant stay to hold you thats the wrong thing to do talk \"\n",
      "to but i cant stay to hold you thats the wrong thing to do talk you and and im and you the for and im and i and the i a im i and im you im and im and \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"to but i cant stay to hold you thats the wrong thing to do talk \"\n",
      "to but i cant stay to hold you thats the wrong thing to do talk a im and bay cap far about it pictures huh it a even its the drinkin enough is and smile like saying ill me they \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"to but i cant stay to hold you thats the wrong thing to do talk \"\n",
      "to but i cant stay to hold you thats the wrong thing to do talk realize iphone napkin okay imma hope damn again time this a barzini fuck i i sweatpants low you trying cause your house lookin such gotti \n",
      "75057/75057 [==============================] - 206s 3ms/sample - loss: 1.5702\n",
      "Epoch 10/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5690----- Generating text after Epoch: 9\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"team man these niggas got to stop it they be crowding up the scene baby \"\n",
      "team man these niggas got to stop it they be crowding up the scene baby you you i you you i you you i i i i you you i i i you you i i i you the i \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"team man these niggas got to stop it they be crowding up the scene baby \"\n",
      "team man these niggas got to stop it they be crowding up the scene baby wanna you that when all game that it but to friends you you i and you dont back for i i first there and chains \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"team man these niggas got to stop it they be crowding up the scene baby \"\n",
      "team man these niggas got to stop it they be crowding up the scene baby hating bar well meet watch was once i problem always word hov the cousin beginna boy slang tryin oh allergic i fresh in yeah its \n",
      "75057/75057 [==============================] - 240s 3ms/sample - loss: 1.5682\n",
      "Epoch 11/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5683----- Generating text after Epoch: 10\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"know that you run shit i get it i get it im workin too hard \"\n",
      "know that you run shit i get it i get it im workin too hard you me to i i i i im i the i i i i i you i i im i i i in i my \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"know that you run shit i get it i get it im workin too hard \"\n",
      "know that you run shit i get it i get it im workin too hard the but seat i too why and it i i in and i what wonder wouldnt you i me you up our that you its \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"know that you run shit i get it i get it im workin too hard \"\n",
      "know that you run shit i get it i get it im workin too hard hardest lucky of more i go its double chef best stone let scared we night even such the he just barzini see looks to go \n",
      "75057/75057 [==============================] - 238s 3ms/sample - loss: 1.5684\n",
      "Epoch 12/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5665----- Generating text after Epoch: 11\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"youll think of me i just really hope that youll think of me i just \"\n",
      "youll think of me i just really hope that youll think of me i just the the the it the the the the it i to the i you the the the i the i the i i the the \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"youll think of me i just really hope that youll think of me i just \"\n",
      "youll think of me i just really hope that youll think of me i just yeah the know you i im and crew the wear you the never if good crew been the on super life a you find for \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"youll think of me i just really hope that youll think of me i just \"\n",
      "youll think of me i just really hope that youll think of me i just proud consciousness get missed only oh song than did that all but bitches two that durrn stone cant hope flying the thats good is down \n",
      "75057/75057 [==============================] - 246s 3ms/sample - loss: 1.5673\n",
      "Epoch 13/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5674----- Generating text after Epoch: 12\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"be your first time they keep telling me dont save you if i ignore all \"\n",
      "be your first time they keep telling me dont save you if i ignore all the a im i me the but the but and a a a im you you a im the but and the and the you \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"be your first time they keep telling me dont save you if i ignore all \"\n",
      "be your first time they keep telling me dont save you if i ignore all little all im you the that in dont i you a know but the i just started cause curse shit take from just come girl \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"be your first time they keep telling me dont save you if i ignore all \"\n",
      "be your first time they keep telling me dont save you if i ignore all im on rock pain forget the youve follow nights wife til kiddin coulda last you friends of ridge then pain thats ruggish was other legend \n",
      "75057/75057 [==============================] - 238s 3ms/sample - loss: 1.5684\n",
      "Epoch 14/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5679----- Generating text after Epoch: 13\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"how that shit go you know how that shit go you know how that shit \"\n",
      "how that shit go you know how that shit go you know how that shit i you i on i i i i in i i you i i i i i i i you i i i i i \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"how that shit go you know how that shit go you know how that shit \"\n",
      "how that shit go you know how that shit go you know how that shit you end can nigga it in done do really it i to i i my window i wave your nights oh i love my your \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"how that shit go you know how that shit go you know how that shit \"\n",
      "how that shit go you know how that shit go you know how that shit yellin staring whats fine i gon it hey cause wonder never roper your questioned could its the better and you shape other dough lovin marathon \n",
      "75057/75057 [==============================] - 248s 3ms/sample - loss: 1.5678\n",
      "Epoch 15/15\n",
      "74752/75057 [============================>.] - ETA: 0s - loss: 1.5656----- Generating text after Epoch: 14\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"too high to be friendly they throw dirt on my name well thats why they \"\n",
      "too high to be friendly they throw dirt on my name well thats why they i you i i i i and i i you i i i the the i the i i i i im i i i \n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"too high to be friendly they throw dirt on my name well thats why they \"\n",
      "too high to be friendly they throw dirt on my name well thats why they like you i i i be muthafucka one a a i on for im i i and i for i the for love i only \n",
      "----- diversity: 0.8\n",
      "----- Generating with seed: \"too high to be friendly they throw dirt on my name well thats why they \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "too high to be friendly they throw dirt on my name well thats why they had maybe spilling yeah time dont what best i what i be im and want had a you what that best bone before shorty fragrance \n",
      "75057/75057 [==============================] - 232s 3ms/sample - loss: 1.5666\n"
     ]
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "history = model.fit(\n",
    "    x, \n",
    "    y,\n",
    "    batch_size=512,\n",
    "    epochs=15,\n",
    "    callbacks=[print_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('drake_word_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed, length, diversity):\n",
    "\n",
    "    maxlen = 15\n",
    "    generated = ''\n",
    "    sentence = tokens[seed: seed + maxlen]\n",
    "    \n",
    "    for i in sentence:\n",
    "        generated += str(i)\n",
    "        generated += ' '\n",
    "\n",
    "    sentence_list = ''\n",
    "    for i in sentence:\n",
    "        sentence_list += str(i) + ' '\n",
    "    print('----- Generating with seed: \"' + sentence_list + '\"')\n",
    "    \n",
    "    sys.stdout.write(generated)\n",
    "\n",
    "    for i in range(length):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "\n",
    "        generated += next_char\n",
    "        sentence.append(next_char)\n",
    "        sentence.pop(0)\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Generating with seed: \"fuckin best you the best i ever had best i ever had best i ever \"\n",
      "fuckin best you the best i ever had best i ever had best i ever 5231\n",
      "today 4143\n",
      "report 941\n",
      "clearing 3192\n",
      "minutes 2049\n",
      "georgia 1509\n",
      "drought 1249\n",
      "days 718\n",
      "c 2705\n",
      "kept 4249\n",
      "roper \n"
     ]
    }
   ],
   "source": [
    "generate_text(model, 1234, 10, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5DlgLLWN4fLcBv+vdUHbA",
   "collapsed_sections": [],
   "name": "lyric_generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
